<html><head><meta name="robots" content="index,follow"><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>kNN classifiers 1. What is a kNN classifier?</title></head><body bgcolor="#FFFFFF">

<table border=0 cellpadding=0 cellspacing=0><tr><td bgcolor="#CCCC00"><table border=4 cellpadding=9><tr><td align=middle bgcolor="#000000"><font face="Palatino,Times" size=6 color="#999900"><b>
kNN classifiers 1. What is a kNN classifier?
</b></font></table></table>
<p>
<i>Instance-based</i> classifiers such as the <i>k</i>NN classifier operate on the premises that classification of unknown instances can be done by relating the unknown to the known according to some <a href="Euclidean_distance.html">distance/similarity function</a>. The intuition is that two instances far apart in the <i>instance space</i> defined by the appropriate <i>distance function</i> are less likely than two closely situated instances to belong to the same class.</p>
<h3>
The learning process</h3>
<p>
Unlike many artificial learners, <i>instance-based</i> learners do not abstract any information from the training data during the learning phase. Learning is merely a question of encapsulating the training data. The process of <i>generalization</i> is postponed until it is absolutely unavoidable, that is, at the time of <i>classification</i>. This property has lead to the referring to <i>instance-based</i> learners as <i>lazy</i> learners, whereas classifiers such as <a href="Feedforward_neural_networks_1__What_is_a_feedforward_ne.html">feedforward neural networks</a>, where proper <i>abstraction</i> is done during the learning phase, often are entitled <i>eager</i> learners.</p>
<h3>
Classification</h3>
<p>
Classification (<i>generalization</i>) using an <i>instance-based</i> classifier can be a simple matter of locating the nearest neighbour in <i>instance space</i> and labelling the unknown instance with the same class label as that of the located (known) neighbour. This approach is often referred to as a <i>nearest neighbour classifier</i>. The downside of this simple approach is the lack of robustness that characterize the resulting classifiers. The high degree of local sensitivity makes <i>nearest neighbour classifiers</i> highly susceptible to noise in the training data.</p>
<p>
More robust models can be achieved by locating <i>k</i>, where <i>k</i> &gt; 1, neighbours and letting the majority vote decide the outcome of the class labelling. A higher value of <i>k</i> results in a smoother, less locally sensitive, function. The <i>nearest neighbour classifier</i> can be regarded as a special case of the more general <i>k-nearest neighbours classifier</i>, hereafter referred to as a <i>k</i>NN classifier. The drawback of increasing the value of <i>k</i> is of course that as <i>k</i> approaches <i>n</i>, where <i>n</i> is the size of the <i>instance base</i>, the performance of the classifier will approach that of the most straightforward <i>statistical baseline</i>, the assumption that all unknown instances belong to the class most most frequently represented in the training data.</p>
<p>
This problem can be avoided by limiting the influence of distant instances. One way of doing so is to assign a weight to each vote, where the weight is a function of the distance between the unknown and the known instance. By letting each weight be defined by the inversed squared distance between the known and unknown instances votes cast by distant instances will have very little influence on the decision process compared to instances in the near neighbourhood. <i>Distance weighted voting</i> usually serves as a good middle ground as far as local sensitivity is concerned.</p>
<h3>Links to this page</h3>
<ul>
<li><a href="k-means_clustering_1__How_does_k-means_clustering_work_.html">k-means clustering 1. How does k-means clustering work?</a>
<li><a href="KNN___Pattern___Categories__Learn___.html">KNN & Pattern & Categories: Learn...</a>
<li><a href="KNN___Pattern___Categories__To_FeatureWeights___.html">KNN & Pattern & Categories: To FeatureWeights...</a>
<li><a href="KNN___Pattern___FeatureWeights__To_Categories___.html">KNN & Pattern & FeatureWeights: To Categories...</a>
<li><a href="KNN___Pattern___FeatureWeights__To_TableOfReal___.html">KNN & Pattern & FeatureWeights: To TableOfReal...</a>
<li><a href="KNN___Pattern__To_Categories___.html">KNN & Pattern: To Categories...</a>
<li><a href="KNN___Pattern__To_TabelOfReal___.html">KNN & Pattern: To TabelOfReal...</a>
<li><a href="kNN_classifiers.html">kNN classifiers</a>
<li><a href="KNN__Get_optimized_parameters___.html">KNN: Get optimized parameters...</a>
<li><a href="KNN__To_FeatureWeights___.html">KNN: To FeatureWeights...</a>
<li><a href="Pattern___Categories__To_KNN_classifier___.html">Pattern & Categories: To KNN classifier...</a>
</ul>
<hr>
<address>
	<p>&copy; Ola SÃ¶der, May 29, 2008</p>
</address>
</body>
</html>
