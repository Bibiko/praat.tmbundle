<html><head><meta name="robots" content="index,follow"><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>expectation-maximization</title></head><body bgcolor="#FFFFFF">

<table border=0 cellpadding=0 cellspacing=0><tr><td bgcolor="#CCCC00"><table border=4 cellpadding=9><tr><td align=middle bgcolor="#000000"><font face="Palatino,Times" size=6 color="#999900"><b>
expectation-maximization
</b></font></table></table>
<p>
Expectation-maximization (EM) is an iterative method used to find maximum likelihood estimates of parameters in probabilistic models, where the model depends on unobserved, also called <i>latent</i>, variables. EM alternates between performing an expectation (E) step, which computes an expectation of the likelihood by including the latent variables as if they were observed, and a maximization (M) step, which computes the maximum likelihood estimates of the parameters by maximizing the expected likelihood found in the E step. The parameters found on the M step are then used to start another E step, and the process is repeated until some criterion is satisfied. EM is frequently used for data clustering like for example in <a href="TableOfReal__To_GaussianMixture___.html">Gaussian mixtures</a> or in the <a href="HMM___HMM_ObservationSequences__Learn___.html">Baum-Welch training</a> of a Hidden Markov Model.</p>
<h3>Links to this page</h3>
<ul>
<li><a href="GaussianMixture___TableOfReal__Improve_likelihood___.html">GaussianMixture & TableOfReal: Improve likelihood...</a>
</ul>
<hr>
<address>
	<p>&copy; djmw, November 30, 2011</p>
</address>
</body>
</html>
